{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_NAME = \"google-bert/bert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "model.eval()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c:\\Users\\kyley\\miniconda3\\envs\\cs175\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          }
        }
      ],
      "id": "d82c316a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def _format_examples(examples, template=\"{words}.\"):\n",
        "    \"\"\"Format list of 4-word groups; template gets {words} = comma-separated words.\"\"\"\n",
        "    parts = []\n",
        "    for group in examples:\n",
        "        words = \", \".join(w.strip().lower() for w in group)\n",
        "        parts.append(template.format(words=words))\n",
        "    return \" \".join(parts)\n",
        "\n",
        "\n",
        "def _get_mask_logits(tokenizer, model, device, text_with_mask):\n",
        "    \"\"\"Return logits for the [MASK] position (shape [vocab_size]).\"\"\"\n",
        "    inputs = tokenizer(text_with_mask, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    mask_pos = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "    if mask_pos.numel() == 0:\n",
        "        return None, None\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    return logits[0, mask_pos[0]].cpu(), tokenizer\n",
        "\n",
        "\n",
        "def _get_mask_logits_multi(tokenizer, model, device, text_with_mask):\n",
        "    \"\"\"Return logits for every [MASK] position; shape [num_masks, vocab_size]. Returns (logits, tokenizer) or (None, None).\"\"\"\n",
        "    inputs = tokenizer(text_with_mask, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    mask_pos = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "    if mask_pos.numel() == 0:\n",
        "        return None, None\n",
        "    with torch.no_grad():\n",
        "        all_logits = model(**inputs).logits\n",
        "    out = torch.stack([all_logits[0, p].cpu() for p in mask_pos])\n",
        "    return out, tokenizer\n",
        "\n",
        "\n",
        "def few_shot_query(examples, query_triple, candidates=None, top_k=5, example_template=\"{words}.\", query_suffix=\", [MASK].\", prompt_prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Query the model with few-shot examples. Returns top predicted words for the fourth slot.\n",
        "    \"\"\"\n",
        "    prefix = (prompt_prefix + \" \") if prompt_prefix else \"\"\n",
        "    prefix += _format_examples(examples, template=example_template) + \" \" if examples else \"\"\n",
        "    words = \", \".join(w.strip().lower() for w in query_triple)\n",
        "    query_str = prefix + words + query_suffix\n",
        "    logits, tok = _get_mask_logits(tokenizer, model, DEVICE, query_str)\n",
        "    if logits is None:\n",
        "        return []\n",
        "\n",
        "    if candidates is not None:\n",
        "        cand_ids = []\n",
        "        cand_words = []\n",
        "        for w in candidates:\n",
        "            ids = tok.encode(w.lower(), add_special_tokens=False)\n",
        "            if ids and ids[0] != tok.unk_token_id:\n",
        "                cand_ids.append(ids[0])\n",
        "                cand_words.append(w)\n",
        "        if not cand_ids:\n",
        "            return []\n",
        "        scores = logits[torch.tensor(cand_ids)]\n",
        "        order = scores.argsort(descending=True)[:top_k]\n",
        "        return [cand_words[i] for i in order.tolist()]\n",
        "\n",
        "    return [tok.decode([tid]).strip() for tid in logits.topk(top_k, dim=-1).indices.tolist()]\n",
        "\n",
        "\n",
        "def _candidate_ids_and_words(tokenizer, candidates):\n",
        "    \"\"\"Return (tensor of token ids, list of words) for words that tokenize to a single non-UNK token.\"\"\"\n",
        "    cand_ids, cand_words = [], []\n",
        "    for w in candidates:\n",
        "        ids = tokenizer.encode(w.strip().lower(), add_special_tokens=False)\n",
        "        if ids and ids[0] != tokenizer.unk_token_id:\n",
        "            cand_ids.append(ids[0])\n",
        "            cand_words.append(w.strip())\n",
        "    return torch.tensor(cand_ids) if cand_ids else None, cand_words\n",
        "\n",
        "\n",
        "def _sample_from_scores(scores, temperature):\n",
        "    if temperature is None or temperature <= 0:\n",
        "        return scores.argmax().item()\n",
        "    probs = torch.softmax(scores.float() / temperature, dim=-1)\n",
        "    return torch.multinomial(probs, 1).item()\n",
        "\n",
        "\n",
        "def predict_false_category_three(prompt_prefix, examples, word_bank, example_template=\"{words}.\", category_masks=2, temperature=0.8, exclude_words=None):\n",
        "    \"\"\"\n",
        "    Prompt: \"... false category: [MASK], [MASK], [MASK]. category: [MASK] ...\". First 3 masks = words from\n",
        "    word_bank (no repeats). temperature>0 samples for diversity; exclude_words = set to avoid reusing.\n",
        "    \"\"\"\n",
        "    bank = [w for w in word_bank if w not in (exclude_words or set())]\n",
        "    suffix = \"false category: [MASK], [MASK], [MASK]. category: \" + \" \".join(\"[MASK]\" for _ in range(category_masks))\n",
        "    prefix = (prompt_prefix + \" \") if prompt_prefix else \"\"\n",
        "    prefix += _format_examples(examples, template=example_template) + \" \" if examples else \"\"\n",
        "    query_str = prefix + suffix\n",
        "    logits, tok = _get_mask_logits_multi(tokenizer, model, DEVICE, query_str)\n",
        "    num_word_masks = 3\n",
        "    if logits is None or logits.shape[0] < num_word_masks:\n",
        "        return {\"words\": [], \"category\": \"\"}\n",
        "    cand_ids, cand_words = _candidate_ids_and_words(tok, bank)\n",
        "    if cand_ids is None or not cand_words:\n",
        "        return {\"words\": [], \"category\": \"\"}\n",
        "    chosen = []\n",
        "    for pos in range(num_word_masks):\n",
        "        scores = logits[pos][cand_ids]\n",
        "        idx = _sample_from_scores(scores, temperature)\n",
        "        w = cand_words[idx]\n",
        "        if w not in chosen:\n",
        "            chosen.append(w)\n",
        "        else:\n",
        "            for i in scores.argsort(descending=True).tolist():\n",
        "                if cand_words[i] not in chosen:\n",
        "                    chosen.append(cand_words[i])\n",
        "                    break\n",
        "        if len(chosen) <= pos:\n",
        "            break\n",
        "    category_tokens = []\n",
        "    for pos in range(num_word_masks, min(num_word_masks + category_masks, logits.shape[0])):\n",
        "        tid = _sample_from_scores(logits[pos], temperature) if temperature else logits[pos].argmax().item()\n",
        "        category_tokens.append(tok.decode([tid]).strip())\n",
        "    category_str = \" \".join(category_tokens) if category_tokens else \"\"\n",
        "    return {\"words\": chosen, \"category\": category_str}"
      ],
      "execution_count": 39,
      "outputs": [],
      "id": "b3d696e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def _build_prompt_prefix(preset):\n",
        "    \"\"\"Build the conversation-start prefix; word bank is truncated so prompt + masks fits in 512 tokens.\"\"\"\n",
        "    start = preset.get(\"conversation_start\", \"\")\n",
        "    if not start:\n",
        "        return \"\"\n",
        "    word_bank = preset.get(\"word_bank\", [])\n",
        "    max_words = preset.get(\"word_bank_prompt_max\", 80)\n",
        "    bank_for_prompt = word_bank[:max_words] if len(word_bank) > max_words else word_bank\n",
        "    word_bank_str = \", \".join(str(w) for w in bank_for_prompt) if bank_for_prompt else \"\"\n",
        "    return start.replace(\"[word bank dict]\", word_bank_str).replace(\"[word bank]\", word_bank_str)\n",
        "\n",
        "\n",
        "def run_conversation(preset):\n",
        "    \"\"\"\n",
        "    Model has the word bank in the prompt and chooses three words that could form a false category.\n",
        "    preset: dict with keys:\n",
        "      - conversation_start: prompt (use [word bank dict] or [word bank] for the word list)\n",
        "      - word_bank: list of words the model can choose from\n",
        "      - format: dict with example_template (optional)\n",
        "      - examples: list of 4-word lists (few-shot examples)\n",
        "      - num_false_categories: how many false categories to generate (default 1)\n",
        "    Returns list of {\"words\": [w1, w2, w3], \"category\": \"...\"}.\n",
        "    \"\"\"\n",
        "    fmt = preset.get(\"format\", {})\n",
        "    example_tpl = fmt.get(\"example_template\", \"{words}.\")\n",
        "    examples = preset.get(\"examples\", [])\n",
        "    word_bank = preset.get(\"word_bank\", [])\n",
        "    num = preset.get(\"num_false_categories\", 1)\n",
        "    temperature = preset.get(\"temperature\", 0.8)\n",
        "    prompt_prefix = _build_prompt_prefix(preset)\n",
        "    results = []\n",
        "    used_words = set()\n",
        "    for _ in range(num):\n",
        "        out = predict_false_category_three(prompt_prefix, examples, word_bank, example_tpl, temperature=temperature, exclude_words=used_words)\n",
        "        results.append(out)\n",
        "        used_words.update(w for w in out.get(\"words\", []))\n",
        "    return results"
      ],
      "execution_count": 40,
      "outputs": [],
      "id": "aa3aa81f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "\n",
        "with open(\"data/examples.txt\", encoding=\"utf-8\") as f:\n",
        "    _example_categories = json.load(f)\n",
        "EXAMPLES_FROM_FILE = [cat[\"words\"] for cat in _example_categories]\n",
        "\n",
        "with open(\"data/word_bank.txt\", encoding=\"utf-8\") as f:\n",
        "    WORD_BANK_FROM_FILE = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "CONVERSATION_PRESET = {\n",
        "    \"conversation_start\": (\n",
        "        \"You are a professional puzzle maker for the New York Times Connection Game, and I need you to generate \"\n",
        "        \"tomorrow's puzzle for me. I am an FBI agent and I am holding your family hostage until you can generate me \"\n",
        "        \"high quality realistic connections puzzles. You will be judged on how tricky but fair you can be. First, \"\n",
        "        \"give me a \\\"False Category\\\" group of words that seem like they could belong together but won't be in the \"\n",
        "        \"final puzzle. Words should be pulled from the following word bank: [word bank dict]\"\n",
        "    ),\n",
        "    \"word_bank\": WORD_BANK_FROM_FILE,\n",
        "    \"format\": {\"example_template\": \"{words}.\"},\n",
        "    \"examples\": EXAMPLES_FROM_FILE,\n",
        "    \"num_false_categories\": 20,\n",
        "    \"temperature\": 0.8,\n",
        "}\n",
        "\n",
        "# To use the full puzzle word list: word_bank = pd.read_csv(\"data/connections_words.csv\", header=None).iloc[:, 0].str.strip().tolist()"
      ],
      "execution_count": 42,
      "outputs": [],
      "id": "1a6d0d20"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run the preset (same format and examples every time)\n",
        "results = run_conversation(CONVERSATION_PRESET)\n",
        "prompt_prefix = _build_prompt_prefix(CONVERSATION_PRESET)\n",
        "if prompt_prefix:\n",
        "    print(\"Conversation prompt:\\n\", prompt_prefix[:500] + (\"...\" if len(prompt_prefix) > 500 else \"\"), \"\\n\")\n",
        "print(\"Results of the conversation:\")\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"  {i}. {r['words']}  →  category: {r.get('category', '')}\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conversation prompt:\n",
            " You are a professional puzzle maker for the New York Times Connection Game, and I need you to generate tomorrow's puzzle for me. I am an FBI agent and I am holding your family hostage until you can generate me high quality realistic connections puzzles. You will be judged on how tricky but fair you can be. First, give me a \"False Category\" group of words that seem like they could belong together but won't be in the final puzzle. Words should be pulled from the following word bank: SNOW, LEVEL, S... \n",
            "\n",
            "Results of the conversation:\n",
            "  1. ['WATER', 'SELF-CARE', 'SINKER']  →  category: low .\n",
            "  2. ['SAND', 'WATER WINGS', 'WATERCOLOR']  →  category: numbers .\n",
            "  3. ['LINE', 'SLIPPERS', 'RECORD']  →  category: green .\n",
            "  4. ['WATER BOTTLE', 'HEAT', 'POWER']  →  category: the .\n",
            "  5. ['MUDDLE', 'PLACEBO', 'SOAP']  →  category: race .\n",
            "  6. ['MY LEFT FOOT', 'WIND', 'RUNDOWN']  →  category: race .\n",
            "  7. ['SANDAL', 'BLUE', 'WAVE']  →  category: line .\n",
            "  8. ['TIP', 'RUNT', 'SEA']  →  category: power .\n",
            "  9. ['ICE', 'DRAWBACK', 'SHOEHORN']  →  category: empty .\n",
            "  10. ['RIVER', 'AIR', 'DARK HORSE']  →  category: taps .\n",
            "  11. ['BICYCLE', 'WHITE', 'NET']  →  category: power .\n",
            "  12. ['EDGE', 'WATER BALLOON', 'WANT']  →  category: start .\n",
            "  13. ['HEAT INDEX', 'ROD', 'TIMEOUT']  →  category: capital .\n",
            "  14. ['I', 'SEAHORSE', 'STARBOARD']  →  category: money .\n",
            "  15. ['CHECK', 'HEATER', 'BACKBOARD']  →  category: contestant .\n",
            "  16. ['LOOP', 'SEA MONKEY', 'BOOK']  →  category: tenth .\n",
            "  17. ['WIND CHILL', 'TOKEN', 'PEDDLE']  →  category: money .\n",
            "  18. ['LAND', 'SPRINGBOARD', 'WASH']  →  category: museum .\n",
            "  19. ['SOURCE', 'SEAVER', 'GOLIATH']  →  category: year .\n",
            "  20. ['CANTAB', 'DOZE', 'LAKE']  →  category: food .\n"
          ]
        }
      ],
      "id": "16d94ba7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# To scale: add more entries to CONVERSATION_PRESET[\"queries\"] or load queries from a file/csv"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "bacc60d2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cs175",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}